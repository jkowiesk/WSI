{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bbe541-ad51-4b9d-975b-69baf42a7162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __WSI Ćwiczenie 5: Q-learning__\n",
    "Autor:\n",
    "Jakub Kowieski\n",
    "\n",
    "Przedmiot:\n",
    "WSI, semestr: 21Z\n",
    "\n",
    "Numer Albumu:\n",
    "310765\n",
    "\n",
    "---\n",
    "Celem ćwiczenia jest implementacja algorytmu Q-learning.\n",
    "## __Założenia:__\n",
    "- Q table - macierz o liczbie kolumn równej liczbie ruchów oraz liczbie wierszy równej liczbie stanów\n",
    "- Gra kończy się gdy agent, zanjdzie się na polu H (hole) lub G (GOAL)\n",
    "- 4 możliwe ruchy\n",
    "    - 0: w lewo\n",
    "    - 1: w dół\n",
    "    - 2: w prawo\n",
    "    - 3: w górę\n",
    "- Są dwa tryby:\n",
    "    1. is_slippery=False - 100% na wykonanie wybranego ruchu\n",
    "    2. is_slippery=True - 33% na wykonanie na wybrany ruch\n",
    "- Parametry agenta:\n",
    "    - alpha - współczynnik zmiany elemntu Q table\n",
    "    - action_prob - prawdopodobieństwo wybrania losowego ruchu\n",
    "    - discount - współczynnik oceny\n",
    "\n",
    "## __Posiadanie modułów:__\n",
    "- gym\n",
    "\n",
    "## __Testowanie:__\n",
    "<ins>*simulate.py*</ins> - symulacja działania algorytmu dla wpisanej liczbie epizodów FronzenLakev1 w trybie is_slippery=True\n",
    "\n",
    "## __Legenda:__\n",
    "- 1 tryb = tryb is_slippery=False\n",
    "- 2 tryb = tryb is_slippery=True\n",
    "\n",
    "# __Q Learning:__\n",
    "\n",
    "Agent na początku porusza się po planszy robiąc losowe ruchy i z każdym epizodem robi je częściej z użyciem Q table. Przy agent.action_prob <= 0.2 agent z dobrze dobranymi parametrami powinien prawie zawsze znajdować dobry ruch.\n",
    "\n",
    "## Działanie Q learningu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9c500-86ef-4ec6-8922-f5961a59962d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "from utils.agent import Agent\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    agent = Agent(env, action_prob=0.5) \n",
    "    counter = 0\n",
    "    \n",
    "    for episode in range(100):\n",
    "        episode += 1\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action_learn(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.learn(state, action, next_state, reward, done)\n",
    "            print(f\"Episode: {episode}, Goals Counter: {counter}, State: {state}, Action: {action}\")\n",
    "            state = next_state\n",
    "            env.render()\n",
    "            print(agent.action_prob)\n",
    "            print(agent.q)\n",
    "            sleep(0.1)\n",
    "            clear_output(wait=True)\n",
    "        if reward == 0:\n",
    "            sleep(0.1)\n",
    "        else:\n",
    "            counter += reward\n",
    "            sleep(0.5)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932595a-9f86-4a99-9228-e7241df2ed81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __Test działania__\n",
    "\n",
    "#### 1. Dla mapy 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782c9e4a-72b0-4159-a2a5-bedc1c4703ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[8.85841561e-01 9.03920797e-01 8.68122568e-01 8.85717763e-01]\n",
      " [8.85842162e-01 7.61367380e-05 8.13581908e-01 8.52750227e-01]\n",
      " [8.59759686e-01 6.98886541e-01 6.63647834e-01 6.53588451e-01]\n",
      " [6.87209176e-01 1.09523774e-01 6.53888060e-01 6.16243329e-01]\n",
      " [9.03917766e-01 9.22368160e-01 2.33374146e-12 8.85840843e-01]\n",
      " [4.74061074e-01 2.12011806e-01 4.03111422e-01 3.20238380e-01]\n",
      " [5.21925887e-02 6.47997061e-01 1.74440430e-01 7.01316153e-01]\n",
      " [8.93309175e-01 5.96745767e-01 4.15873389e-01 7.89310660e-01]\n",
      " [9.22223201e-01 9.25291450e-07 9.41192000e-01 9.03919979e-01]\n",
      " [9.21616731e-01 9.60400000e-01 9.59204275e-01 1.29463091e-06]\n",
      " [6.29772989e-01 9.79999942e-01 1.60241861e-01 6.83737073e-01]\n",
      " [9.97650941e-01 9.14796183e-01 3.61603435e-01 1.31215809e-01]\n",
      " [8.01091758e-02 9.90838341e-01 6.13712427e-01 7.95777525e-01]\n",
      " [1.90941018e-04 9.60396242e-01 9.80000000e-01 9.41178920e-01]\n",
      " [9.59526873e-01 9.37784533e-01 1.00000000e+00 9.60363862e-01]\n",
      " [9.86758599e-01 4.84551750e-01 4.07501534e-02 5.85083416e-01]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "from utils.agent import Agent\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    agent = Agent(env)\n",
    "\n",
    "    for episode in range(300):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action_learn(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.learn(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "\n",
    "    counter = 0\n",
    "    for test in range(1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            print(f\"State: {state}, Action: {action}\")\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            counter += reward\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "            clear_output(wait=True)\n",
    "    print(counter)\n",
    "    print(agent.q)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b2cb3-6af0-4bed-8b81-f15d5266a79f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Dla mapy 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e634244-bbca-4cbc-884a-8c236ea878a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "from utils.agent import Agent\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"8x8\")\n",
    "    agent = Agent(env)\n",
    "\n",
    "    for episode in range(400):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action_learn(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.learn(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "\n",
    "    counter = 0\n",
    "    for test in range(1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            print(f\"State: {state}, Action: {action}\")\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            counter += reward\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "            clear_output(wait=True)\n",
    "    print(counter)\n",
    "    print(agent.q)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7aac-6826-422e-9c97-abe92c5e08b8",
   "metadata": {},
   "source": [
    "#### 3. Wynik simulate.py\n",
    "\n",
    "![wynik simulate](assets/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d18d44-30dc-4042-beb9-33193cbae674",
   "metadata": {},
   "source": [
    "# __Wnioski:__\n",
    "\n",
    "Algorytm działa bardzo dobrze dla trybu pierwszego, oraz sensownie dla drugiego. Jest to spowodowane losowością w 2 trybie, przy każdym ruchu jest tylko 33% szans na wykonanie wybranego ruchu. Wszystkie parametry agenta mają zakres od (0, 1). Parametr alpha oraz action_prob zależy od ilości epizodów oraz rozmiaru planszy przy uczeniu agenta. Przy małej ilości epizodów i rozmiaru planszy, należy proporcjonalnie zwiększyć parametry, bo inaczej agent nie zdąży się nauczyć. Za to parametr discount nie ma ogromnego znaczenia, jednakże przy dużych planszach nie może być on zbyt mały, bo wartości Q table przy polu G będą nie znaczące dla sumy ocen dla początkowych stanów. Początkowe stany nie mają po prostu wystarczającego zasięgu, który ustawia nam discount."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
